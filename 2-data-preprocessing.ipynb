{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0b3bd099373ac6196c54af3604d576270990f67f45fdd913a085e54ebe5a36e3f",
   "display_name": "Python 3.9.2 64-bit ('CapstoneProject': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Credit Card Fraud Detection\n",
    "by **Sai Vamsy Dhulipala**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part II - Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing necessary packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "source": [
    "## Importing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Defining a function to read the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    file = pickle.load(open(getcwd() + \"/data/cleaned_data/\" + name + \".pkl\",\"rb\"))\n",
    "    return file"
   ]
  },
  {
   "source": [
    "### Reading the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = input(\"Please provide the path of the folder, by replacing \\\\ as /:\")\n",
    "chdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data(\"train\")\n",
    "valid = read_data(\"valid\")\n",
    "test = read_data(\"test\")"
   ]
  },
  {
   "source": [
    "## Preprocessing data for model building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 1. Splitting the data into X and y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"is_fraud\", axis=1)\n",
    "y_train = train[\"is_fraud\"]\n",
    "\n",
    "X_valid = valid.drop(\"is_fraud\", axis=1)\n",
    "y_valid = valid[\"is_fraud\"]\n",
    "\n",
    "X_test = test.drop(\"is_fraud\", axis=1)\n",
    "y_test = test[\"is_fraud\"]"
   ]
  },
  {
   "source": [
    "#### 2. Applying PCA to preserve 98% of the variance in the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Kindly note that the above %age of variance to preserve has been selected after performing Grid Search CV on logistic regression, which has not been included in the notebook due to computational constraints."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.98)\n",
    "pca = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_decomposed = pca.transform(X_train)\n",
    "X_valid_decomposed = pca.transform(X_valid)\n",
    "X_test_decomposed = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of train set: (907672, 68)\nShape of valid set: (389003, 68)\nShape of test set: (555719, 68)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of train set: {X_train_decomposed.shape}\")\n",
    "print(f\"Shape of valid set: {X_valid_decomposed.shape}\")\n",
    "print(f\"Shape of test set: {X_test_decomposed.shape}\")"
   ]
  },
  {
   "source": [
    "#### 3. Resampling the data to handle class imbalance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "SMOTEENN technique (Over-sampling using SMOTE and Under-sampling using ENN (Edited Nearest Neighbours)) has been used here since there is heavy class imbalance and the size of the data is quite huge. SMOTEENN has provided with an icrease of 7% in recall compared to SMOTE and ADASYN techniques. Building and evaluating the models using SMOTE and ADASYN has not been included in the notebook due to computational constraints.\n",
    "\n",
    "Also note that resampling has been done on the valid set but not on the train set, since valid set itself took approximately 12 hours to be resampled. Hence, the pickle files of the resampled data have also been attached in the submission files in order to save time. The code to load the files has been provided below in markdown. Please use the same code while running the notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "resampler = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = resampler.fit_resample(X_valid_decomposed, y_valid)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "source": [
    "def read_resampled_data(name):\n",
    "    file = pickle.load(open(getcwd() + \"/data/resampled_data/\" + name + \"_resampled.pkl\",\"rb\"))\n",
    "    return file\n",
    "\n",
    "X_resampled = read_resampled_data(\"X\")\n",
    "y_resampled = read_resampled_data(\"y\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({1: 386751, 0: 380700})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_resampled))"
   ]
  },
  {
   "source": [
    "## Saving the data into pickle files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, file_name in (X_train_decomposed, \"X_train_decomposed\"), (X_valid_decomposed, \"X_valid_decomposed\"), (X_test_decomposed, \"X_test_decomposed\"):\n",
    "    pickle.dump(df, open(getcwd() + \"/data/decomposed_data/\" + file_name + \".pkl\", \"wb\"))\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, file_name in (y_train, \"y_train\"), (y_valid, \"y_valid\"), (y_test, \"y_test\"):\n",
    "    pickle.dump(df, open(getcwd() + \"/data/decomposed_data/\" + file_name + \".pkl\", \"wb\"))\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, file_name in (X_resampled, \"X_resampled\"), (y_resampled, \"y_resampled\"):\n",
    "    pickle.dump(df, open(getcwd() + \"/data/resampled_data/\" + file_name + \".pkl\", \"wb\"))\n",
    "    del df"
   ]
  },
  {
   "source": [
    "### Deleting all the variables present in the memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ]
}